# MultiScan: Scalable RGBD scanning for 3D environments with articulated objects

<h4>
    <a href="https://3dlg-hcvc.github.io/multiscan/">Project</a> |
    <a href="https://openreview.net/forum?id=YxUdazpgweG">Paper</a> |
    <a href="https://spathi.cmpt.sfu.ca/projects/multiscan/multiscan-docs/index.html">Docs</a>
</h4>

MultiScan is a scalable RGBD dataset construction pipeline leveraging commodity mobile devices to scan indoor scenes with articulated objects and web-based semantic annotation interfaces to efficiently annotate object and part semantics and part mobility parameters. 

<img src="docs/read-the-docs/_static/teaser.png" />

The repository includes:

* Source code of iOS and Android scanning apps
* Processing server for 3D reconstruction, texturing and segmentation
* Web interface for browsing scans and initiate processings
* Source code of data visualization and preprocessing

-----------------------------------

## MultiScan Dataset

Dataset download:
```bash
./dataset/download.sh output_dir
```

The downloaded dataset would follow this [file system structure](https://spathi.cmpt.sfu.ca/projects/multiscan/multiscan-docs/dataset/index.html#file-system-structure).

MultiScan data includes:
1. Acquired data from scanner app: [doc](https://spathi.cmpt.sfu.ca/projects/multiscan/multiscan-docs/dataset/files/acquired.html)
2. Output data from processing server: [doc](https://spathi.cmpt.sfu.ca/projects/multiscan/multiscan-docs/dataset/files/output.html)
3. Annotation data: [doc](https://spathi.cmpt.sfu.ca/projects/multiscan/multiscan-docs/dataset/files/annotation.html)

-----------------------------------

## Scanner App

The Scanner App collects data using sensors on an Android/iOS device. User moves around holding the device with Scanner app installed to scan the scene. Once the scanning is completed, users can upload the data to the processing server.
* source code for iOS scanning app: [iOS code](scanner/ios)
* documentation for iOS scanning app: [iOS doc](https://spathi.cmpt.sfu.ca/projects/multiscan/multiscan-docs/scanner/index.html#ios)
* source code for Android scanning app: [Android code](scanner/android)
* documentation for Android scanning app: [Android doc](https://spathi.cmpt.sfu.ca/projects/multiscan/multiscan-docs/scanner/index.html#android)

-----------------------------------

## Processing Server
The staging server has 3 main functionalities:

1. Stage uploaded scans by the devices (iOS or Android) and trigger scan processing. To ensure that scans can be automatically processed, the scans should be placed in a directory with lots of space and accessible to the scanning processor.
2. Process staged scans. Handle reconstruction processing request from Web-UI, when user press interactive buttons on Web-UI.
3. Index staged scans. Go through scan folders and collate information about the scans.
* source code for processing server: [server code](server)
* installation doc for processing server: [install](https://spathi.cmpt.sfu.ca/projects/multiscan/multiscan-docs/server/index.html#installation)
* configurations and documentations for processing server: [doc](https://spathi.cmpt.sfu.ca/projects/multiscan/multiscan-docs/server/index.html#configurations)


### Staging Data Formats

Details about the formats of the uploaded files, and data generated by the processing server are available at [**here**](./docs/staging_file_format.md)

-----------------------------------

## Web-UI

The Web-UI is an interactive interface for providing an overview of staged scan data, managing scan data, and controlling the reconstruction and mesh annotation pipeline.
- source code to webui server backend: [web-server code](web-ui/web-server)
- source code to webui client frontend: [web-client code](web-ui/web-client)
- Web-UIinstallation and usuage documentation: [doc](https://spathi.cmpt.sfu.ca/projects/multiscan/multiscan-docs/web-ui/index.html#web-ui)

-----------------------------------

## Data processing and visualization

Coming soon.

-----------------------------------

## Citation

If you use the MultiScan data or code please cite:

```
```

## References

Our work is built on top of the ScanNet dataset acquisition framework, Open3D, and MVS-Texturing for 3D reconstruction.
We use the Open3D, Pyrender, MeshLab and Instant Meshes for rendering and post-processing.

    @misc{dai2017scannet,
      title={ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes}, 
      author={Angela Dai and Angel X. Chang and Manolis Savva and Maciej Halber and Thomas Funkhouser and Matthias Nie√üner},
      year={2017},
      eprint={1702.04405},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
    }

    @article{Zhou2018,
        author    = {Qian-Yi Zhou and Jaesik Park and Vladlen Koltun},
        title     = {{Open3D}: {A} Modern Library for {3D} Data Processing},
        journal   = {arXiv:1801.09847},
        year      = {2018},
    }

    @inproceedings{Waechter2014Texturing,
        title    = {Let There Be Color! --- {L}arge-Scale Texturing of {3D} Reconstructions},
        author   = {Waechter, Michael and Moehrle, Nils and Goesele, Michael},
        booktitle= {Proceedings of the European Conference on Computer Vision},
        year     = {2014},
        publisher= {Springer},
    }

    @article{jakob2015instant,
        author = {Wenzel Jakob and Marco Tarini and Daniele Panozzo and Olga Sorkine-Hornung},
        title = {Instant Field-Aligned Meshes},
        journal = TOG_SIGA,
        volume = {34},
        number = {6},
        year = {2015},
        month = nov,
        doi = {10.1145/2816795.2818078},
    }

    @software{meshlab,
        author       = {Paolo Cignoni, Alessandro Muntoni, Guido Ranzuglia, Marco Callieri},
        title        = {{MeshLab}},
        publisher    = {Zenodo},
        doi          = {10.5281/zenodo.5114037}
    }
# Staging File Format

By default the staging data are store in `/path/to/staging/scanID` directory.
## Upload files

```shell
<scanId>
----------------------------------------------------------------------
Data uploaded by scanner app

|-- <scanId>.json
|-- <scanId>.jsonl
|-- <scanId>.depth.zlib
|-- <scanId>.confidence.zlib
|-- <scanId>.mp4
```
-----------------------------------

### \<scanID\>.json
Contains the metadata for the current scene. [Example file](./example_files/example_json.json)

```c#
- device:
  - id (string)
  - type (string)
  - name (string)
- scene:
  - type (string)
  - gps_location ([float])
  - description (string)
- depth_unit (string)
- depth_confidence_available (boolean)
- depth_confidence_value_range ([int])
- camera_orientation_quaternion_format (string)
- camera_orientation_euler_angles_format (string)
- number_of_files (int)
- streams: ([object]) Note: There are two types of objects in streams
  Type 1:
    - extrinsics
  - frequency (int)
  - number_of_frames (int)
  - id (string)
  - file_extensions (string)
  - intrinsics ([float])
  - type (string)
  - resolution ([int]) Note: The format is [height, width]
  - encoding (string)
  Type 2:
    - id (string)
    - encoding
    - number_of_frames (int)
    - type (string)
    - frequency (int)
    - file_extension (string)
- user:
  - name (string)
```

### \<scanID\>.jsonl
Contains camera details **for each frame** [Example file](./example_files/example_jsonl.json)
```c#
- quaternion ([float])
- timestamp (int)
- intrinsics ([float])
- transform ([float])
- euler_angles ([float])
- exposure_duration (int)
```

* `intrinsics` is a 9 by 1 vector corresponds to 3x3 intrinsics matrix for RGB frames in column major. The instrinsics matrix for depth frames can be obtained by scale the `intrinsics` like [code line 216 to 224](https://github.com/3dlg-hcvc/multiscan/blob/09ac771d03dd42b7593c1cec6dec82b0e1fa6e0c/reconstruction/scripts/bridge.py#L216), and transform it into a 3x3 intrinsics matrix.

* `transform` is a 16 by 1 vector corresponds to 4x4 6 DoF camera pose matrix in column major. According to [ARKit transform](https://developer.apple.com/documentation/arkit/arcamera/2866108-transform), the x-axis points from the front-facing camera toward the Home button. The y-axis points upward, and the z-axis points away from the device on the screen side.   
To convert it into 4x4 camera pose matrix, please look at [code line 207 to 210](https://github.com/3dlg-hcvc/multiscan/blob/ff8c06ed1d8e6140f821b5b41e884a4fb2ce71b0/reconstruction/scripts/bridge.py#L207) for reference.  
[line 212](https://github.com/3dlg-hcvc/multiscan/blob/ff8c06ed1d8e6140f821b5b41e884a4fb2ce71b0/reconstruction/scripts/bridge.py#L212) convert the pose to Open3D coordinates.   
Inverse the calculated 4x4 pose matrix, to convert it into camera extrinsics matrix.

* `euler_angles` express the camera orientation in roll, pitch, yaw values. The x,y,z axis is same as described in `transform` above. The order is stored in `camera_orientation_euler_angles_format` in `scanID.json`. By default, it is "x,y,z".
* `quaternion` the order is stored in `camera_orientation_quaternion_format` in `scanID.json`. By default, it is "w,x,y,z".

* `timestamp` an integer value indicates the time the frame was captured. [ARKit timestamp](https://developer.apple.com/documentation/arkit/arframe/2867973-timestamp)
* `exposure_duration` Duration of the camera exposure, used to effect motion blur. [ARKit exposureDuration](https://developer.apple.com/documentation/arkit/arcamera/3182986-exposureduration)


### \<scanID\>.depth.zlib
The iOS app records depth as 16-bit float values in meters, the Android app records depth as 16-bit unsigned shortvalues in millimetres.  
The raw depth frames are stream compressed using zlib compression.

### \<scanID\>.confidence.zlib
Confidence map pixles are 8-bit unsigned integers.
The raw confidence maps are stream compressed using zlib compression.

### \<scanID\>.mp4
Recorded RGB frames is encoded with H.264 codec.

## Processed data files

```shell
<scanId>
----------------------------------------------------------------------
Data generated by server

|-- <scanId>_preview.mp4
    Compressed RGB color stream with lower resolution
|-- <scanId>_thumb.jpg
    A thumbnail RGB frame extracted from <scanId>.mp4
|-- <scanId>_o3d_thumb(2).png
    Rendering result of reconstructed and decimated mesh in low and high reslutions
|-- <scanId>_obj_thumb(2).png
    Rendering result of textured mesh in low and high reslutions
|-- <scanId>.ply
    Coordinate aligned raw reconstructed mesh
|-- <scanId>_decimated.ply
    Coordinate aligned, cleaned and decimatted mesh from the raw reconstructed mesh
|-- <scanId>-align-transform.ply
    The 6DoF pose of the reconstructed mesh before coordinate alignment
|-- <scanId>-.%6f.segs.json
    Hierachical segmentation output of the decimated mesh
|-- <scanId>-.%6f.segs.ply/.png
    Visualization of the hierachical segmentation output of the decimated mesh
|-- textured_mesh/texturedMesh.obj, texturedMesh.mtl, texture_%04d.png
    Textured mesh from decimatted mesh
|-- textured_mesh/texturedMesh.ply
    Mesh with vertex colors converted from textured  mesh
|-- textured_mesh/texturedMesh.%6f.segs.json
    Hierachical segmentation output of the textured mesh
|-- textured_mesh/texturedMesh.%6f.segs.ply/.png
    Visualization of the hierachical segmentation output of the textured mesh
|--process.log
    Logging information of the processing
|--processed.txt
    The processing status of each step of the pipeline
```
-----------------------------------